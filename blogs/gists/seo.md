# SEO

[TOC]

搜索引擎优化包含了对网站内容的基本元素的一系列改动，这些改动通常相互作用，提高用户体验以及增加网站在自然搜索结果中的排名。

#### SEO 专家

SEO 专家的工作和开发者所作的不同，他们通常会给出系统的方案，比如：

* 审核网站的结构和内容
* 给出技术相关的建议，比如托管、重定向、错误页等
* 关键字研究
* 培训
* 垂直行业、特定地域下的专业知识

#### 禁止抓取

我一直以为 Robots.txt 中的 Disallow 是代表禁止搜索引擎抓取相关页面，但是看了 Google Search 的文档才知道，在特定情况下，谷歌会突破 Disalloow 的限制，并把相关网页编入索引，也就是包含到搜索结果中。

> 尽管谷歌不会抓取被 robots.txt 文件屏蔽的内容或将其编入索引，但如果网络上的其他位置有链接指向被禁止访问的网址，我们仍可能会找到该网址并将其编入索引。
> <name>[谷歌搜索中心](https://developers.google.com/search/docs/advanced/robots/intro#understand-the-limitations-of-a-robots.txt-file)</name>

文档中这句应该是个病句，“尽管...不会将其编入索引，但...，仍可能将其编入索引”。而且最令人尴尬的是，他所说的“特定情况”，一点都不特殊，只要互联网中任何一个地方包含了 Disallow 页面的链接，那么谷歌就会突破禁止指令限制。

我最近正着手把我的博客和我的记事本（或者说笔记）统一在一起，所以我的博客中不仅放了我的博客，还有一堆我不想展示给搜索引擎的“低质量”内容。可是我必须给这些页面留下导航，给我自己，以及一些实在想看这些内容的朋友进入页面的机会。很显然，这样会打破 Robots.txt 的禁止指令的限制。

所以如果要完全禁止谷歌搜索引擎将其编入索引，需要使用 HTML meta-noindex 标签或 X-Robots-Tag Header。


#### 谷歌搜索新手指南

谷歌的 SEO 新手指南给出了一些具体的建议，对理解搜索引擎及 SEO 的具体工作很有用。需要强调的是，SEO 新手指南中很大部分都是站在用户的角度去给出提高网站用户体验的建议。这里收集一些平时可能较少关注的点。

受欢迎的内容：

* 👍 搜索引擎在抓取某个网页时，应该和普通用户看到的网页相同。
* 👍 使用数据标注工具和结构化数据标记助手等工具完善页面的富媒体标记，并跟踪已标记页面的表现。
* 👍 为用户创建导航页，为搜索引擎创建站点地图。
* 👍 在网址中使用字词而不是各种 ID。
* 👍 文档和网址应该是一对一的关系，如果多个网址需要访问同一个文档，使用重定向或“rel='canonical'”是不错的选择。
* 👍 撰写简单易懂的文字，用户喜欢文笔优秀、易于理解的内容。
* 👍 定位文字至少提供有关链接到的网页的基本说明。
* 👍 使用 nofollow 处理垃圾评论。
* 👍 使用 RSS、邮件或其它方法推广您的网站。
* 👍 与相关社区的其他网站建立联系。

不受欢迎的内容：

* 👎 允许抓取内部搜索或因代理服务而创建的结果页。
* 👎 具有深层嵌套的子目录或与内容无关的目录结构。
* 👎 文笔欠佳，编写的文章马虎草率，有许多拼写和语法错误。
* 👎 在您的网页上投放会分散用户注意力的广告。
* 👎 使用宽泛的定位文字，如“网页”、“文章”或“点击此处”。
* 👎 在多数情况下将网页的网址用作定位文字。虽然这样做在某些情况下也合情合理，如宣传或引用新网站的地址。