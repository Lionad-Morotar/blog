# SEO

[TOC]

搜索引擎优化包含了对网站内容的基本元素的一系列改动，这些改动通常相互作用，提高用户体验以及增加网站在自然搜索结果中的排名。

#### SEO 专家

SEO 专家的工作和开发者所作的不同，他们通常会给出系统的方案，比如：

* 审核网站的结构和内容
* 给出技术相关的建议，比如托管、重定向、错误页等
* 关键字研究
* 培训
* 垂直行业、特定地域下的专业知识

#### 禁止抓取

我一直以为 Robots.txt 中的 Disallow 是代表禁止搜索引擎抓取相关页面，但是看了 Google Search 的文档才知道，在特定情况下，Google 会突破 Disalloow 的限制，并把相关网页编入索引，也就是包含到搜索结果中。

> 尽管 Google 不会抓取被 robots.txt 文件屏蔽的内容或将其编入索引，但如果网络上的其他位置有链接指向被禁止访问的网址，我们仍可能会找到该网址并将其编入索引。
> <name>[Google 搜索中心](https://developers.google.com/search/docs/advanced/robots/intro#understand-the-limitations-of-a-robots.txt-file)</name>

文档中这句应该是个病句，“尽管...不会将其编入索引，但...，仍可能将其编入索引”。而且最令人尴尬的是，他所说的“特定情况”，一点都不特殊，只要互联网中任何一个地方包含了 Disallow 页面的链接，那么谷歌就会突破禁止指令限制。

我最近正着手把我的博客和我的记事本（或者说笔记）统一在一起，所以我的博客中不仅放了我的博客，还有一堆我不想展示给搜索引擎的“低质量”内容。可是我必须给这些页面留下导航，给我自己，以及一些实在想看这些内容的朋友进入页面的机会。很显然，这样会打破 Robots.txt 的禁止指令的限制。

所以如果要完全禁止 Google 搜索引擎将其编入索引，需要使用 HTML meta-noindex 标签或 X-Robots-Tag Header。
