(window.webpackJsonp=window.webpackJsonp||[]).push([[191],{981:function(t,r,a){"use strict";a.r(r);var e=a(0),_=Object(e.a)({},(function(){var t=this,r=t._self._c;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"爬虫与反爬虫"}},[t._v("爬虫与反爬虫")]),t._v(" "),r("nav",{staticClass:"table-of-contents"},[r("ol",[r("li",[r("a",{attrs:{href:"#爬虫与反爬虫"}},[t._v("爬虫与反爬虫")]),r("ol",[r("li",[r("a",{attrs:{href:"#技术问题"}},[t._v("技术问题")]),r("ol",[r("li",[r("a",{attrs:{href:"#有哪些常见的反爬虫策略"}},[t._v("有哪些常见的反爬虫策略？")])]),r("li",[r("a",{attrs:{href:"#js-加密如何破解"}},[t._v("JS 加密如何破解？")])]),r("li",[r("a",{attrs:{href:"#多进程-协程-多线程如何选择"}},[t._v("多进程、协程、多线程如何选择？")])]),r("li",[r("a",{attrs:{href:"#增量爬取是怎么设计的"}},[t._v("增量爬取是怎么设计的？")])]),r("li",[r("a",{attrs:{href:"#如何判断页面的相似性"}},[t._v("如何判断页面的相似性？")])]),r("li",[r("a",{attrs:{href:"#架构技巧"}},[t._v("架构技巧？")])]),r("li",[r("a",{attrs:{href:"#如何测试网站的抓取极限"}},[t._v("如何测试网站的抓取极限？")])])])]),r("li",[r("a",{attrs:{href:"#调试工具"}},[t._v("调试工具")])]),r("li",[r("a",{attrs:{href:"#links"}},[t._v("Links")])])])])])]),r("h2",{attrs:{id:"技术问题"}},[t._v("技术问题")]),t._v(" "),r("h4",{attrs:{id:"有哪些常见的反爬虫策略"}},[t._v("有哪些常见的反爬虫策略？")]),t._v(" "),r("p",[t._v("前端主要从 HTML、CSS、JS 的角度去考虑。HTML 和 CSS 的策略较简单，比如 iframe 延迟加载、隐形内容、background 偏移、字体重映射等。JS 加密一般会用于价值较高的项目。服务器端可以分主动防御和被动防御两项，指纹、蜜罐、无头浏览器校验、UA、IP、robots.txt 等等。")]),t._v(" "),r("p",[t._v("通过对规则进行组合可以设计出更严格的的反爬策略，如：先访问某图片才能访问某 URL，否则被计分。")]),t._v(" "),r("h4",{attrs:{id:"js-加密如何破解"}},[t._v("JS 加密如何破解？")]),t._v(" "),r("p",[t._v("主要依靠调试，找到加解密的关键 JS 函数。")]),t._v(" "),r("h4",{attrs:{id:"多进程、协程、多线程如何选择"}},[t._v("多进程、协程、多线程如何选择？")]),t._v(" "),r("p",[t._v("爬虫在抓取阶段时是网络密集的，解析数据阶段是 CPU 密集的，写入数据阶段是 IO 密集的。由于大部分开销都花费在了抓取阶段，所以宜选择多线程或协程的方式写爬虫程序。更好的方式可能是非阻塞 IO（异步 IO）。")]),t._v(" "),r("h4",{attrs:{id:"增量爬取是怎么设计的"}},[t._v("增量爬取是怎么设计的？")]),t._v(" "),r("p",[t._v("通过网址管理器管理每个 URL 的状态。它维护了如 URL 是否被下载完、下载失败次数、子页面等数据，并能根据需求返回仍未被抓取的页面 URL。")]),t._v(" "),r("h4",{attrs:{id:"如何判断页面的相似性"}},[t._v("如何判断页面的相似性？")]),t._v(" "),r("p",[t._v("使用 Google 的 SmithHash 算法。")]),t._v(" "),r("p",[t._v("见："),r("a",{attrs:{href:"https://www.likecs.com/show-204424165.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("SimHash 算法原理")])]),t._v(" "),r("h4",{attrs:{id:"架构技巧"}},[t._v("架构技巧？")]),t._v(" "),r("p",[t._v("爬虫分为抓取、解析和写入三个阶段，所以可以拆分为抓取写入以及读取解析两个程序，分别部署，提高应用以及维护的效率。")]),t._v(" "),r("h4",{attrs:{id:"如何测试网站的抓取极限"}},[t._v("如何测试网站的抓取极限？")]),t._v(" "),r("p",[t._v("如果测试每分钟抓 100 次被封了，那么很可能在 80 次就上了监控，所以可以把频次控制在 40 次左右，防止别人的爬虫触发了目标站的反爬虫限制（如 80 次上监控进化为 60 次）。")]),t._v(" "),r("h2",{attrs:{id:"调试工具"}},[t._v("调试工具")]),t._v(" "),r("ul",[r("li",[r("a",{attrs:{href:"https://github.com/adriancooney/puppeteer-heap-snapshot",target:"_blank",rel:"noopener noreferrer"}},[t._v("puppeteer-heap-snapshot")]),t._v("，根据属性的名字遍历堆内存，查找有这些属性的对象。")])]),t._v(" "),r("h2",{attrs:{id:"links"}},[t._v("Links")]),t._v(" "),r("p",[r("a",{attrs:{href:"https://www.yuanrenxue.com/crawler/why-write-python-crawler.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("TODO，猿人学爬虫教程")])])])}),[],!1,null,null,null);r.default=_.exports}}]);